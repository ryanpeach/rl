{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters were taken from https://github.com/yc930401/Actor-Critic-pytorch/blob/master/Actor-Critic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f881078a950>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN = [128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "CATEGORICAL_ACTION = True\n",
    "ACTION_DIM = env.action_space.n\n",
    "STATE_DIM = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "N_ITERS = 1000\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor Critic methods use two network outputs:\n",
    "\n",
    "1. A value prediction based on the state. (Value Network, Critic)\n",
    "2. An action prediction based on the state. (Policy Network, Actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        # We are going to make a feed forward network of depth len(HIDDEN)\n",
    "        self.layers = []\n",
    "        self.input_layer = nn.Linear(STATE_DIM, HIDDEN[0])\n",
    "        for i, j in zip(HIDDEN, HIDDEN[1:]):\n",
    "            self.layers.append(nn.Linear(i,j))\n",
    "            \n",
    "        # These will be our output layers\n",
    "        # If the policy is categorical we need to use a softmax output\n",
    "        self.policy_output_layer = nn.Linear(HIDDEN[-1], ACTION_DIM)\n",
    "\n",
    "    def forward(self, state):\n",
    "        temp = F.relu(self.input_layer(state))\n",
    "        for layer in self.layers:\n",
    "            temp = F.relu(layer(temp))\n",
    "            \n",
    "        # The policy network \n",
    "        # If the policy is categorical we need to sample from the softmax distribution\n",
    "        policy_dist = self.policy_output_layer(temp)\n",
    "        if CATEGORICAL_ACTION:\n",
    "            policy_dist = F.softmax(policy_dist, dim=0)\n",
    "            \n",
    "        return policy_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        # We are going to make a feed forward network of depth len(HIDDEN)\n",
    "        self.layers = []\n",
    "        self.input_layer_state = nn.Linear(STATE_DIM, HIDDEN[0])\n",
    "        self.input_layer_action = nn.Linear(ACTION_DIM, HIDDEN[0])\n",
    "        for i, j in zip(HIDDEN, HIDDEN[1:]):\n",
    "            self.layers.append(nn.Linear(i,j))\n",
    "            \n",
    "        # These will be our output layers\n",
    "        # If the policy is categorical we need to use a softmax output\n",
    "        self.value_output_layer = nn.Linear(HIDDEN[-1], 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_input = F.relu(self.input_layer_state(state))\n",
    "        action_input = F.relu(self.input_layer_action(action))\n",
    "        temp = state_input + action_input\n",
    "        for layer in self.layers:\n",
    "            temp = F.relu(layer(temp))\n",
    "            \n",
    "        # The value output is a simple linear layer\n",
    "        value = self.value_output_layer(temp)\n",
    "        \n",
    "        # Return as a tuple\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1551d686fa0f4267842a53b7da76f374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter', 'uid': 'd564a08c-9017-45bb-ab8e-204e3edad8f4'}], 'layout': {'tâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.FigureWidget()\n",
    "fig.add_scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-d6e84a44e02b>:24: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actor, critic = Actor(), Critic()\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=LR)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=LR)\n",
    "scores = []\n",
    "video_recorder = VideoRecorder(env, './output/03_Cartpole_A2C_Q_Critic.mp4', enabled=True)\n",
    "for this_iter in range(N_ITERS):\n",
    "    log_probs, values, rewards, dones = [], [], [], []\n",
    "    entropy = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for i in count():\n",
    "        # Rendering\n",
    "        if (this_iter+1) % 10 == 0:\n",
    "            video_recorder.capture_frame()\n",
    "\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        policy_dist = actor(state)\n",
    "        policy = Categorical(policy_dist)\n",
    "        action = policy.sample()\n",
    "        value = critic(state, policy_dist.detach())\n",
    "        next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "        log_prob = policy.log_prob(action).unsqueeze(0)\n",
    "        entropy += policy.entropy().mean()\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "        dones.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # print('Iteration: {}, Score: {}'.format(this_iter, i))\n",
    "            scores.append(i)\n",
    "            with fig.batch_update():\n",
    "                # fig.data[0].x = list(range(this_iter))\n",
    "                for i in range(len(scores)):\n",
    "                    fig.data[0].y = scores\n",
    "            break\n",
    "\n",
    "    log_probs = torch.cat(log_probs)\n",
    "    values = torch.cat(values)\n",
    "    rewards = torch.cat(rewards)\n",
    "    dones = torch.cat(dones)\n",
    "    \n",
    "    # REF: https://github.com/yc930401/Actor-Critic-pytorch/blob/master/Actor-Critic.py\n",
    "    # Get the predicted value at the final state\n",
    "    final_value = critic(torch.FloatTensor(state).to(device), policy_dist.detach())\n",
    "\n",
    "    # Calculate the cumulative rewards using the final predicted value as the terminal value\n",
    "    cum_reward = final_value\n",
    "    not_dones = 1 - dones\n",
    "    discounted_future_rewards = torch.FloatTensor(np.zeros(len(rewards))).to(device)\n",
    "    for i in range(len(rewards)):\n",
    "        cum_reward = rewards[-i] + GAMMA * cum_reward * not_dones[-1]\n",
    "        discounted_future_rewards[-i] = cum_reward\n",
    "\n",
    "    # Now we calculate the advantage function\n",
    "    advantage = discounted_future_rewards - values\n",
    "\n",
    "    # And the loss for both the actor and the critic\n",
    "    actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "        \n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "    # torch.save(model, 'model.pkl')\n",
    "video_recorder.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl)",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}